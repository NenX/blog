(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{385:function(e,t,a){"use strict";a.r(t);var s=a(42),r=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"_1-9-important-themes"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-9-important-themes"}},[e._v("#")]),e._v(" 1.9 Important Themes")]),e._v(" "),a("p",[e._v("This concludes our initial whirlwind tour of systems. An important idea to take\naway from this discussion is that a system is more than just hardware. It is a\ncollection of intertwined hardware and systems software that must cooperate in\norder to achieve the ultimate goal of running application programs. The rest of\nthis book will fill in some details about the hardware and the software, and it will\nshow how, by knowing these details, you can write programs that are faster, more\nreliable, and more secure.")]),e._v(" "),a("p",[e._v("To close out this chapter, we highlight several important concepts that cut\nacross all aspects of computer systems. We will discuss the importance of these\nconcepts at multiple places within the book.")]),e._v(" "),a("h2",{attrs:{id:"_1-9-1-amdahl-s-law"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-9-1-amdahl-s-law"}},[e._v("#")]),e._v(" 1.9.1 Amdahl’s Law")]),e._v(" "),a("p",[e._v("Amdahl’s law describes a general principle for improving any process. In\naddition to its application to speeding up computer systems, it can guide a company\ntrying to reduce the cost of manufacturing razor blades, or a student trying to\nimprove his or her grade point average. Perhaps it is most meaningful in the world\n60 Chapter 1 A Tour of Computer Systems of computers, where we routinely improve performance by factors of 2 or more.\nSuch high factors can only be achieved by optimizing large parts of a system.")]),e._v(" "),a("h2",{attrs:{id:"_1-9-2-concurrency-and-parallelism"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-9-2-concurrency-and-parallelism"}},[e._v("#")]),e._v(" 1.9.2 Concurrency and Parallelism")]),e._v(" "),a("p",[e._v("Throughout the history of digital computers, two demands have been constant\nforces in driving improvements: we want them to do more, and we want them to\nrun faster. Both of these factors improve when the processor does more things at\nonce. We use the term concurrency to refer to the general concept of a system with\nmultiple, simultaneous activities, and the term parallelism to refer to the use of\nconcurrency to make a system run faster. Parallelism can be exploited at multiple\nlevels of abstraction in a computer system. We highlight three levels here, working\nfrom the highest to the lowest level in the system hierarchy.")]),e._v(" "),a("h3",{attrs:{id:"thread-level-concurrency"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#thread-level-concurrency"}},[e._v("#")]),e._v(" Thread-Level Concurrency")]),e._v(" "),a("p",[e._v("Building on the process abstraction, we are able to devise systems where multiple\nprograms execute at the same time, leading to "),a("strong",[e._v("concurrency")]),e._v(". With threads, we\ncan even have multiple control flows executing within a single process. Support\nfor concurrent execution has been found in computer systems since the advent\nof time-sharing in the early 1960s. Traditionally, this concurrent execution was\nonly "),a("strong",[e._v("simulated")]),e._v(", by having a single computer rapidly switch among its executing\nprocesses, much as a juggler keeps multiple balls flying through the air. This form\nof concurrency allows multiple users to interact with a system at the same time,\nsuch as when many people want to get pages from a single Web server. It also\nallows a single user to engage in multiple tasks concurrently, such as having a\nWeb browser in one window, a word processor in another, and streaming music\nplaying at the same time. Until recently, most actual computing was done by a\nsingle processor, even if that processor had to switch among multiple tasks. This\nconfiguration is known as a "),a("strong",[e._v("uniprocessor system")]),e._v(".")]),e._v(" "),a("p",[e._v("When we construct a system consisting of multiple processors all under the\ncontrol of a single operating system kernel, we have a "),a("strong",[e._v("multiprocessor system")]),e._v(".\nSuch systems have been available for large-scale computing since the 1980s, but\nthey have more recently become commonplace with the advent of "),a("strong",[e._v("multi-core processors")]),e._v("\nand "),a("strong",[e._v("hyperthreading")]),e._v(". Figure 1.16 shows a taxonomy of these different\nprocessor types.")]),e._v(" "),a("p",[e._v("Multi-core processors have several CPUs (referred to as “cores”) integrated\nonto a single integrated-circuit chip. Figure 1.17 illustrates the organization of a typical multi-core processor,\nwhere the chip has four CPU cores, each with its own L1 and L2 caches,\nand with each L1 cache split into two parts—one to hold recently fetched instructions and one to hold data.\nThe cores share higher levels of cache as well as the interface to main memory.\nIndustry experts predict that they will be able to have dozens, and ultimately hundreds, of cores on a single chip.")]),e._v(" "),a("p",[e._v("Hyperthreading, sometimes called simultaneous multi-threading, is a technique that allows a single CPU to execute multiple flows of control. It involves having multiple copies of some of the CPU hardware, such as program counters and register files, while having only single copies of other parts of the hardware, such as the units that perform floating-point arithmetic. Whereas a conventional processor requires around 20,000 clock cycles to shift between different threads, a hyperthreaded processor decides which of its threads to execute on a cycle-bycycle basis. It enables the CPU to take better advantage of its processing resources. For example, if one thread must wait for some data to be loaded into a cache, the CPU can proceed with the execution of a different thread. As an example, the Intel Core i7 processor can have each core executing two threads, and so a four-core system can actually execute eight threads in parallel.")]),e._v(" "),a("p",[e._v("The use of multiprocessing can improve system performance in two ways.\nFirst, it reduces the need to simulate concurrency when performing multiple tasks.\nAs mentioned, even a personal computer being used by a single person is expected\nto perform many activities concurrently. Second, it can run a single application\nprogram faster, but only if that program is expressed in terms of multiple threads\nthat can effectively execute in parallel. Thus, although the principles of concurrency have been formulated and studied for over 50 years, the advent of multi-core\nand hyperthreaded systems has greatly increased the desire to find ways to write application programs that can exploit the thread-level parallelism available with the hardware. Chapter 12 will look much more deeply into concurrency and its use to provide a sharing of processing resources and to enable more parallelism\nin program execution.")]),e._v(" "),a("h3",{attrs:{id:"instruction-level-parallelism"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instruction-level-parallelism"}},[e._v("#")]),e._v(" Instruction-Level Parallelism")]),e._v(" "),a("p",[e._v("At a much lower level of abstraction, modern processors can execute multiple\ninstructions at one time, a property known as "),a("strong",[e._v("instruction-level parallelism")]),e._v(". For\nexample, early microprocessors, such as the 1978-vintage Intel 8086, required\nmultiple (typically 3–10) clock cycles to execute a single instruction. More recent\nprocessors can sustain execution rates of 2–4 instructions per clock cycle. Any\ngiven instruction requires much longer from start to finish, perhaps 20 cycles or\nmore, but the processor uses a number of clever tricks to process as many as 100\ninstructions at a time. In Chapter 4, we will explore the use of "),a("strong",[e._v("pipelining")]),e._v(", where the\nactions required to execute an instruction are partitioned into different steps and\nthe processor hardware is organized as a series of stages, each performing one\nof these steps. The stages can operate in parallel, working on different parts of\ndifferent instructions. We will see that a fairly simple hardware design can sustain\nan execution rate close to 1 instruction per clock cycle.")]),e._v(" "),a("p",[e._v("Processors that can sustain execution rates faster than 1 instruction per cycle\nare known as** superscalar processors**. Most modern processors support superscalar\noperation. In Chapter 5, we will describe a high-level model of such processors.\nWe will see that application programmers can use this model to understand the\nperformance of their programs. They can then write programs such that the generated code achieves higher degrees of instruction-level parallelism and therefore\nruns faster.")]),e._v(" "),a("h3",{attrs:{id:"single-instruction-multiple-data-simd-parallelism"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#single-instruction-multiple-data-simd-parallelism"}},[e._v("#")]),e._v(" Single-Instruction, Multiple-Data (SIMD) Parallelism")]),e._v(" "),a("p",[e._v("At the lowest level, many modern processors have special hardware that allows\na single instruction to cause multiple operations to be performed in parallel, a\nmode known "),a("strong",[e._v("assingle-instruction, multiple-data (SIMD)")]),e._v(" parallelism. For example,\nrecent generations of Intel and AMD processors have instructions that can add 8\npairs of single-precision floating-point numbers (C data type float) in parallel.")]),e._v(" "),a("p",[e._v("These SIMD instructions are provided mostly to speed up applications that\nprocess image, sound, and video data. Although some compilers attempt to automatically extract SIMD parallelism from C programs, a more reliable method is to\nwrite programs using special vector data types supported in compilers such as gcc.\nWe describe this style of programming in Web Aside opt:simd, as a supplement to\nthe more general presentation on program optimization found in Chapter 5.")]),e._v(" "),a("h2",{attrs:{id:"_1-9-3-the-importance-of-abstractions-in-computer-systems"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-9-3-the-importance-of-abstractions-in-computer-systems"}},[e._v("#")]),e._v(" 1.9.3 The Importance of Abstractions in Computer Systems")]),e._v(" "),a("p",[e._v("The use of abstractions is one of the most important concepts in computer science.\nFor example, one aspect of good programming practice is to formulate a simple\napplication program interface (API) for a set of functions that allow programmers\nto use the code without having to delve into its inner workings. Different programing languages provide different forms and levels of support for abstraction, such\nas Java class declarations and C function prototypes.")]),e._v(" "),a("p",[e._v("We have already been introduced to several of the abstractions seen in computer systems, as indicated in Figure 1.18. On the processor side, the instruction set\narchitecture provides an abstraction of the actual processor hardware. With this\nabstraction, a machine-code program behaves as if it were executed on a processor that performs just one instruction at a time. The underlying hardware is far\nmore elaborate, executing multiple instructions in parallel, but always in a way\nthat is consistent with the simple, sequential model. By keeping the same execution model, different processor implementations can execute the same machine\ncode while offering a range of cost and performance.")]),e._v(" "),a("p",[e._v("On the operating system side, we have introduced three abstractions: files as\nan abstraction of I/O devices, virtual memory as an abstraction of program memory, and processes as an abstraction of a running program. To these abstractions\nwe add a new one: the virtual machine, providing an abstraction of the entire\ncomputer, including the operating system, the processor, and the programs. The\nidea of a virtual machine was introduced by IBM in the 1960s, but it has become\nmore prominent recently as a way to manage computers that must be able to run\nprograms designed for multiple operating systems (such as Microsoft Windows,\nMac OS X, and Linux) or different versions of the same operating system.")]),e._v(" "),a("p",[e._v("We will return to these abstractions in subsequent sections of the book.")])])}),[],!1,null,null,null);t.default=r.exports}}]);