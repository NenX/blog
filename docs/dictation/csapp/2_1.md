# 2.1 Information Storage

Rather than accessing individual bits in memory, most computers use blocks of
8 bits, or bytes, as the smallest addressable unit of memory. A machine-level
program views memory as a very large array of bytes, referred to as **virtual memory**. 
Every byte of memory is identified by a unique number, known as its
address, and the set of all possible addresses is known as the **virtual address space**.
As indicated by its name, this virtual address space is just a conceptual image
presented to the machine-level program. The actual implementation (presented
in Chapter 9) uses a combination of dynamic random access memory (DRAM),
flash memory, disk storage, special hardware, and operating system software to
provide the program with what appears to be a monolithic byte array.

In subsequent chapters, we will cover how the compiler and run-time system
partitions this memory space into more manageable units to store the different
program objects, that is, program data, instructions, and control information.
Various mechanisms are used to allocate and manage the storage for different
parts of the program. This management is all performed within the virtual address
space. For example, the value of a pointer in C—whether it points to an integer,
a structure, or some other program object—is the virtual address of the first byte
of some block of storage. The C compiler also associates type information with
each pointer, so that it can generate different machine-level code to access the
value stored at the location designated by the pointer depending on the type of
that value. Although the C compiler maintains this type information, the actual
machine-level program it generates has no information about data types. It simply
treats each program object as a block of bytes and the program itself as a sequence
of bytes.

## 2.1.1 Hexadecimal Notation

A single byte consists of 8 bits. In binary notation, its value ranges from 00000000<sub>2</sub>
to 11111111<sub>2</sub>. When viewed as a decimal integer, its value ranges from 0<sub>10</sub> to 255<sub>10</sub>.
Neither notation is very convenient for describing bit patterns. Binary notation
is too verbose, while with decimal notation it is tedious to convert to and from
bit patterns. Instead, we write bit patterns as base-16, or **hexadecimal** numbers.
Hexadecimal (or simply “hex”) uses digits ‘0’ through ‘9’ along with characters
‘A’ through ‘F’ to represent 16 possible values. Figure 2.2 shows the decimal and
binary values associated with the 16 hexadecimal digits. Written in hexadecimal,
the value of a single byte can range from 00<sub>16</sub> to FF<sub>16</sub>.

In C, numeric constants starting with `0x` or `0X` are interpreted as being in
hexadecimal. The characters ‘A’ through ‘F’ may be written in either upper- or
lowercase. For example, we could write the number FA1D37B<sub>16</sub> as `0xFA1D37B`, as
`0xfa1d37b`, or even mixing upper- and lowercase (e.g., `0xFa1D37b`). We will use
the C notation for representing hexadecimal values in this book.

A common task in working with machine-level programs is to manually convert between decimal, binary, and hexadecimal representations of bit patterns. Converting between binary and hexadecimal is straightforward, since it can be
performed one hexadecimal digit at a time. Digits can be converted by referring
to a chart such as that shown in Figure 2.2. One simple trick for doing the conversion in your head is to memorize the decimal equivalents of hex digits A, C, and F.The hex values B, D, and E can be translated to decimal by computing their values
relative to the first three.

|               |      |      |      |      |      |      |      |      |
| ------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Hex digit     | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
| Decimal value | 0    | 1    | 2    | 3    | 4    | 5    | 6    | 7    |
| Binary value  | 0000 | 0001 | 0010 | 0011 | 0100 | 0101 | 0110 | 0111 |
| Hex digit     | 8    | 9    | A    | B    | C    | D    | E    | F    |
| Decimal value | 8    | 9    | 10   | 11   | 12   | 13   | 14   | 15   |
| Binary value  | 1000 | 1001 | 1010 | 1011 | 1100 | 1101 | 1110 | 1111 |

For example, suppose you are given the number 0x173A4C. You can convert
this to binary format by expanding each hexadecimal digit, as follows:

|             |      |      |      |      |      |      |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- |
| Hexadecimal | 1    | 7    | 3    | A    | 4    | C    |
| Binary      | 0001 | 0111 | 0011 | 1010 | 0100 | 1100 |

This gives the binary representation 000101110011101001001100.

Conversely, given a binary number 1111001010110110110011, you convert it
to hexadecimal by first splitting it into groups of 4 bits each. Note, however, that if
the total number of bits is not a multiple of 4, you should make the leftmost group
be the one with fewer than 4 bits, effectively padding the number with leading
zeros. Then you translate each group of bits into the corresponding hexadecimal
digit:

|             |     |      |      |      |      |      |
| ----------- | --- | ---- | ---- | ---- | ---- | ---- |
| Binary      | 11  | 1100 | 1010 | 1101 | 1011 | 0011 |
| Hexadecimal | 3   | C    | A    | D    | B    | 3    |

## 2.1.2 Data Sizes

Every computer has a word size, indicating the nominal size of pointer data. Since
a virtual address is encoded by such a word, the most important system parameter
determined by the word size is the maximum size of the virtual address space. That
is, for a machine with a w-bit word size, the virtual addresses can range from 0 to
2<sup>w</sup> − 1, giving the program access to at most 2<sup>w</sup> bytes.


In recent years, there has been a widespread shift from machines with 32-
bit word sizes to those with word sizes of 64 bits. This occurred first for high-end
machines designed for large-scale scientific and database applications, followed
by desktop and laptop machines, and most recently for the processors found in
smartphones. A 32-bit word size limits the virtual address space to 4 gigabytes
(written 4 GB), that is, just over 4 × 10<sup>9</sup> bytes. Scaling up to a 64-bit word size
leads to a virtual address space of 16 exabytes, or around 1.84 × 10<sup>19</sup> bytes.

Most 64-bit machines can also run programs compiled for use on 32-bit machines, a form of backward compatibility. So, for example, when a program prog.c
is compiled with the directive

```bash
linux> gcc -m32 prog.c
```

then this program will run correctly on either a 32-bit or a 64-bit machine. On the
other hand, a program compiled with the directive

```bash
linux> gcc -m64 prog.c
```

will only run on a 64-bit machine. We will therefore refer to programs as being
either “32-bit programs” or “64-bit programs,” since the distinction lies in how a
program is compiled, rather than the type of machine on which it runs.

Computers and compilers support multiple data formats using different ways
to encode data, such as integers and floating point, as well as different lengths.
For example, many machines have instructions for manipulating single bytes, as
well as integers represented as 2-, 4-, and 8-byte quantities. They also support
floating-point numbers represented as 4- and 8-byte quantities.

The C language supports multiple data formats for both integer and floating-point data. 
Figure 2.3 shows the number of bytes typically allocated for different C
data types. (We discuss the relation between what is guaranteed by the C standard
versus what is typical in Section 2.2.) The exact numbers of bytes for some data
types depends on how the program is compiled. We show sizes for typical 32-bit
and 64-bit programs. Integer data can be either signed, able to represent negative,
zero, and positive values, or unsigned, only allowing nonnegative values. Data
type char represents a single byte. Although the name char derives from the fact
that it is used to store a single character in a text string, it can also be used to store
integer values. Data types short, int, and long are intended to provide a range of
sizes. Even when compiled for 64-bit systems, data type int is usually just 4 bytes.
Data type long commonly has 4 bytes in 32-bit programs and 8 bytes in 64-bit
programs

| Signed        | Unsigned       | 32-bit | 64-bit |
| ------------- | -------------- | ------ | ------ |
| [signed] char | unsigned char  | 1      | 1      |
| short         | unsigned short | 2      | 2      |
| int           | unsigned       | 4      | 4      |
| long          | unsigned long  | 4      | 8      |
| int32_t       | uint32_t       | 4      | 4      |
| int64_t       | uint64_t       | 8      | 8      |
| char *        |                | 4      | 8      |
| float         |                | 4      | 4      |
| double        |                | 8      | 8      |


To avoid the vagaries of relying on “typical” sizes and different compiler settings, ISO C99 introduced a class of data types where the data sizes are fixed
regardless of compiler and machine settings. Among these are data types int32_t
and int64_t, having exactly 4 and 8 bytes, respectively. Using fixed-size integer
types is the best way for programmers to have close control over data representations.

Most of the data types encode signed values, unless prefixed by the keyword
unsigned or using the specific unsigned declaration for fixed-size data types. The
exception to this is data type char. Although most compilers and machines treat
these as signed data, the C standard does not guarantee this. Instead, as indicated
by the square brackets, the programmer should use the declaration signed char
to guarantee a 1-byte signed value. In many contexts, however, the program’s
behavior is insensitive to whether data type char is signed or unsigned.

The C language allows a variety of ways to order the keywords and to include
or omit optional keywords. As examples, all of the following declarations have
identical meaning:

```cpp
unsigned long a;
unsigned long int b;
long unsigned c;
long unsigned int d;
```

We will consistently use the forms found in Figure 2.3.

Figure 2.3 also shows that a pointer (e.g., a variable declared as being of
type char *) uses the full word size of the program. Most machines also support
two different floating-point formats: single precision, declared in C as float,
and double precision, declared in C as double. These formats use 4 and 8 bytes,
respectively.

Programmers should strive to make their programs portable across differentmachines and compilers. 
One aspect of portability is to make the program insensitive to the exact sizes of the different data types. 
The C standards set lower bounds on the numeric ranges of the different data types, as will be covered later, but there are no upper bounds (except with the fixed-size types). With 32-bit machines and 32-bit programs being the dominant combination from around 1980 until around 2010, 
many programs have been written assuming the allocations listed for 32-bit programs in Figure 2.3. With the transition to 64-bit machines, 
many hidden word size dependencies have arisen as bugs in migrating these programs to new
machines. For example, many programmers historically assumed that an object
declared as type int could be used to store a pointer. This works fine for most
32-bit programs, but it leads to problems for 64-bit programs.

## 2.1.3 Addressing and Byte Ordering

For program objects that span multiple bytes, we must establish two conventions:
what the address of the object will be, and how we will order the bytes in memory.
In virtually all machines, a multi-byte object is stored as a contiguous sequence
of bytes, with the address of the object given by the smallest address of the bytes
used. For example, suppose a variable x of type int has address 0x100; that is, the
value of the address expression &x is 0x100. Then (assuming data type int has a
32-bit representation) the 4 bytes of x would be stored in memory locations 0x100,
0x101, 0x102, and 0x103.

For ordering the bytes representing an object, there are two common conventions. Consider a w-bit integer having a bit representation [x<sub>w−1</sub>, x<sub>w−2</sub>,...,x<sub>1</sub>, x<sub>0</sub>], where xw−1 is the most significant bit and x0 is the least. Assuming w is a multiple of 8, these bits can be grouped as bytes, with the most significant byte having bits [x<sub>w−1</sub>, x<sub>w−2</sub>,...,x<sub>w−8</sub>], the least significant byte having bits [x<sub>7</sub>, x<sub>6</sub>,...,x<sub>0</sub>], and
the other bytes having bits from the middle. Some machines choose to store the object in memory ordered from least significant byte to most, while other machines
store them from most to least. The former convention—where the least significant
byte comes first—is referred to as **little endian**. The latter convention—where the
most significant byte comes first—is referred to as **big endian**.

Suppose the variable x of type int and at address 0x100 has a hexadecimal
value of 0x01234567. The ordering of the bytes within the address range 0x100
through 0x103 depends on the type of machine:

- Big endian
| 0x100 | 0x101 | 0x102 | 0x103 |
| ----- | ----- | ----- | ----- |
| 01    | 23    | 45    | 67    |
- Little endian
| 0x100 | 0x101 | 0x102 | 0x103 |
| ----- | ----- | ----- | ----- |
| 67    | 45    | 23    | 01    |

Note that in the word 0x01234567 the high-order byte has hexadecimal value
0x01, while the low-order byte has value 0x67.

Most Intel-compatible machines operate exclusively in little-endian mode. On the other hand, 
most machines from IBM and Oracle (arising from their acquisition of Sun Microsystems in 2010) operate in big-endian mode. 
Note that we said “most.” The conventions do not split precisely along corporate boundaries. For example, 
both IBM and Oracle manufacture machines that use Intel-compatible processors and hence are little endian. Many recent microprocessor chips are
bi-endian, meaning that they can be configured to operate as either little- or
big-endian machines. In practice, however, byte ordering becomes fixed once a
particular operating system is chosen. For example, ARM microprocessors, used
in many cell phones, have hardware that can operate in either little- or big-endian
mode, but the two most common operating systems for these chips—Android
(from Google) and IOS (from Apple)—operate only in little-endian mode.


People get surprisingly emotional about which byte ordering is the proper one.
In fact, the terms “little endian” and “big endian” come from the book Gulliver’s
Travels by Jonathan Swift, where two warring factions could not agree as to how a
soft-boiled egg should be opened—by the little end or by the big. Just like the egg
issue, there is no technological reason to choose one byte ordering convention over
the other, and hence the arguments degenerate into bickering about sociopolitical
issues. As long as one of the conventions is selected and adhered to consistently,
the choice is arbitrary.

For most application programmers, the byte orderings used by their machines
are totally invisible; programs compiled for either class of machine give identical results. At times, however, byte ordering becomes an issue. 
The first is when binary data are communicated over a network between different machines. 
A common problem is for data produced by a little-endian machine to be sent to
a big-endian machine, or vice versa, leading to the bytes within the words being
in reverse order for the receiving program. To avoid such problems, code written
for networking applications must follow established conventions for byte ordering to make sure the sending machine converts its internal representation to the
network standard, while the receiving machine converts the network standard to
its internal representation. We will see examples of these conversions in Chapter 11.

A second case where byte ordering becomes important is when looking at
the byte sequences representing integer data. This occurs often when inspecting
machine-level programs. As an example, the following line occurs in a file that
gives a text representation of the machine-level code for an Intel x86-64 processor:

```
4004d3: 01 05 43 0b 20 00 add %eax,0x200b43(%rip)
```

This line was generated by a disassembler, a tool that determines the instruction
sequence represented by an executable program file. We will learn more about
disassemblers and how to interpret lines such as this in Chapter 3. For now, we
simply note that this line states that the hexadecimal byte sequence 01 05 43 0b
20 00 is the byte-level representation of an instruction that adds a word of data
to the value stored at an address computed by adding 0x200b43 to the current
value of the program counter, the address of the next instruction to be executed.
If we take the final 4 bytes of the sequence 43 0b 20 00 and write them in reverse
order, we have 00 20 0b 43. Dropping the leading 0, we have the value 0x200b43,
the numeric value written on the right. Having bytes appear in reverse order
is a common occurrence when reading machine-level program representations
generated for little-endian machines such as this one. The natural way to write a
byte sequence is to have the lowest-numbered byte on the left and the highest on
the right, but this is contrary to the normal way of writing numbers with the most
significant digit on the left and the least on the right.


A third case where byte ordering becomes visible is when programs are
written that circumvent the normal type system. In the C language, this can be
done using a cast or a union to allow an object to be referenced according to
a different data type from which it was created. Such coding tricks are strongly
discouraged for most application programming, but they can be quite useful and
even necessary for system-level programming.

Figure 2.4 shows C code that uses casting to access and print the byte representations of different program objects. We use typedef to define data type
byte_pointer as a pointer to an object of type unsigned char. Such a byte pointer
references a sequence of bytes where each byte is considered to be a nonnegative integer. The first routine show_bytes is given the address of a sequence of
bytes, indicated by a byte pointer, and a byte count. The byte count is specified as
having data type size_t, the preferred data type for expressing the sizes of data
structures. It prints the individual bytes in hexadecimal. The C formatting directive %.2x indicates that an integer should be printed in hexadecimal with at least
2 digits.

```c
#include <stdio.h>

typedef unsigned char *byte_pointer;

void show_bytes(byte_pointer start, size_t len)
{
    int i;
    for (i = 0; i < len; i++)
        printf(" %.2x", start[i]);
    printf("\n");
}

void show_int(int x)
{
    show_bytes((byte_pointer)&x, sizeof(int));
}

void show_float(float x)
{
    show_bytes((byte_pointer)&x, sizeof(float));
}

void show_pointer(void *x)
{
    show_bytes((byte_pointer)&x, sizeof(void *));
}
```

Procedures show_int, show_float, and show_pointer demonstrate how to
use procedure show_bytes to print the byte representations of C program objects
of type int, float, and void *, respectively. Observe that they simply pass show_
bytes a pointer &x to their argument x, casting the pointer to be of type unsigned
char *. This cast indicates to the compiler that the program should consider the
pointer to be to a sequence of bytes rather than to an object of the original data
type. This pointer will then be to the lowest byte address occupied by the object.

These procedures use the C sizeof operator to determine the number of bytes
used by the object. In general, the expression sizeof(T ) returns the number of
bytes required to store an object of type T . Using sizeof rather than a fixed value
is one step toward writing code that is portable across different machine types.

We ran the code shown in Figure 2.5 on several different machines, giving the
results shown in Figure 2.6. The following machines were used:

- Linux 32 Intel IA32 processor running Linux.
- Windows Intel IA32 processor running Windows.
- Sun Sun Microsystems SPARC processor running Solaris. (These machines are now produced by Oracle.)
- Linux 64 Intel x86-64 processor running Linux.


```c
void test_show_bytes(int val) 
{
    int ival = val;
    float fval = (float) ival;
    int *pval = &ival;
    show_int(ival);
    show_float(fval);
    show_pointer(pval);
}
```

| Machine  | Value    | Type  | Bytes  (hex)            |
| -------- | -------- | ----- | ----------------------- |
| Linux 32 | 12,345   | int   | 39 30 00 00             |
| Windows  | 12,345   | int   | 39 30 00 00             |
| Sun      | 12,345   | int   | 00 00 30 39             |
| Linux 64 | 12,345   | int   | 39 30 00 00             |
| Linux 32 | 12,345.0 | float | 00 e4 40 46             |
| Windows  | 12,345.0 | float | 00 e4 40 46             |
| Sun      | 12,345.0 | float | 46 40 e4 00             |
| Linux 64 | 12,345.0 | float | 00 e4 40 46             |
| Linux 32 | &ival    | int * | e4 f9 ff bf             |
| Windows  | &ival    | int * | b4 cc 22 00             |
| Sun      | &ival    | int * | ef ff fa 0c             |
| Linux 64 | &ival    | int * | b8 11 e5 ff ff 7f 00 00 |

Our argument 12,345 has hexadecimal representation 0x00003039. For the int
data, we get identical results for all machines, except for the byte ordering. In
particular, we can see that the least significant byte value of 0x39 is printed first
for Linux 32, Windows, and Linux 64, indicating little-endian machines, and last
for Sun, indicating a big-endian machine. Similarly, the bytes of the float data
are identical, except for the byte ordering. On the other hand, the pointer values
are completely different. The different machine/operating system configurations
use different conventions for storage allocation. One feature to note is that the
Linux 32, Windows, and Sun machines use 4-byte addresses, while the Linux 64
machine uses 8-byte addresses.

Observe that although the floating-point and the integer data both encode
the numeric value 12,345, they have very different byte patterns: 0x00003039
for the integer and 0x4640E400 for floating point. In general, these two formats
use different encoding schemes. If we expand these hexadecimal patterns into
binary form and shift them appropriately, we find a sequence of 13 matching bits,
indicated by a sequence of asterisks, as follows:

```
   0   0   0   0   3   0   3   9
00000000000000000011000000111001
                   *************
          01000110010000001110010000000000
             4   6   4   0   E   4   0   0
```

This is not coincidental. We will return to this example when we study floatingpoint formats.

## 2.1.4 Representing Strings

A string in C is encoded by an array of characters terminated by the null (having
value 0) character. Each character is represented by some standard encoding, with
the most common being the ASCII character code. Thus, if we run our routine
show_bytes with arguments "12345" and 6 (to include the terminating character),
we get the result 31 32 33 34 35 00. Observe that the ASCII code for decimal digit
x happens to be 0x3x, and that the terminating byte has the hex representation
0x00. This same result would be obtained on any system using ASCII as its
character code, independent of the byte ordering and word size conventions. As
a consequence, text data are more platform independent than binary data.

## 2.1.5 Representing Code

Consider the following C function:

```c
int sum(int x, int y)
{
    return x + y;
}
```
When compiled on our sample machines, we generate machine code having
the following byte representations:

- Linux 32: 55 89 e5 8b 45 0c 03 45 08 c9 c3
- Windows:55 89 e5 8b 45 0c 03 45 08 5d c3
- Sun:81 c3 e0 08 90 02 00 09
- Linux 64: 55 48 89 e5 89 7d fc 89 75 f8 03 45 fc c9 c3

Here we find that the instruction codings are different. Different machine types
use different and incompatible instructions and encodings. Even identical processors running different operating systems have differences in their coding conventions and hence are not binary compatible. Binary code is seldom portable across different combinations of machine and operating system.

A fundamental concept of computer systems is that a program, from the
perspective of the machine, is simply a sequence of bytes. The machine has no
information about the original source program, except perhaps some auxiliary
tables maintained to aid in debugging. We will see this more clearly when we study
machine-level programming in Chapter 3.

## 2.1.6 Introduction to Boolean Algebra

Since binary values are at the core of how computers encode, store, and manipulate information, a rich body of mathematical knowledge has evolved around the
study of the values 0 and 1. This started with the work of George Boole (1815–
1864) around 1850 and thus is known as Boolean algebra. Boole observed that by
encoding logic values true and false as binary values 1 and 0, he could formulate
an algebra that captures the basic principles of logical reasoning.

The simplest Boolean algebra is defined over the two-element set {0, 1}.
Figure 2.7 defines several operations in this algebra. Our symbols for representing
these operations are chosen to match those used by the C bit-level operations,as will be discussed later. The Boolean operation ~ corresponds to the logical
operation not, denoted by the symbol ¬. That is, we say that ¬P is true when
P is not true, and vice versa. Correspondingly, ~p equals 1 when p equals 0, and
vice versa. Boolean operation & corresponds to the logical operation and, denoted
by the symbol ∧. We say that P ∧ Q holds when both P is true and Q is true.
Correspondingly, p & q equals 1 only when p = 1 and q = 1. Boolean operation
| corresponds to the logical operation or, denoted by the symbol ∨. We say that
P ∨ Q holds when either P is true or Q is true. Correspondingly, p | q equals
1 when either p = 1 or q = 1. Boolean operation ^ corresponds to the logical
operation exclusive-or, denoted by the symbol ⊕. We say that P ⊕ Q holds when
either P is true or Q is true, but not both. Correspondingly, p ^ q equals 1 when
either p = 1 and q = 0, or p = 0 and q = 1.

Claude Shannon (1916–2001), who later founded the field of information
theory, first made the connection between Boolean algebra and digital logic. In
his 1937 master’s thesis, he showed that Boolean algebra could be applied to the
design and analysis of networks of electromechanical relays. Although computer
technology has advanced considerably since, Boolean algebra still plays a central
role in the design and analysis of digital systems.

We can extend the four Boolean operations to also operate on bit vectors,
strings of zeros and ones of some fixed length w. We define the operations over bit
vectors according to their applications to the matching elements of the arguments.
Let a and b denote the bit vectors [aw−1, aw−2,...,a0] and [bw−1, bw−2,...,b0],
respectively. We define a & b to also be a bit vector of length w, where the ith
element equals ai & bi, for `0 ≤ i<w`. The operations |, ^, and ~ are extended to
bit vectors in a similar fashion.

As examples, consider the case where w = 4, and with arguments a = `[0110]`
and b = `[1100]`. Then the four operations a & b, a | b, a ^ b, and ~b yield

One useful application of bit vectors is to represent finite sets. We can encode
any subset A ⊆ {0, 1,...,w − 1} with a bit vector [aw−1,...,a1, a0], where ai = 1if
and only if i ∈ A. For example, recalling that we write aw−1 on the left and a0 on the
right, bit vector a = `[01101001]`encodes the set A = {0, 3, 5, 6}, while bit vector b =
`[01010101]`encodes the setB = {0, 2, 4, 6}. With this way of encoding sets, Boolean
operations | and & correspond to set union and intersection, respectively, and ~
corresponds to set complement. Continuing our earlier example, the operation
a & b yields bit vector `[01000001]`, while A ∩ B = {0, 6}.

We will see the encoding of sets by bit vectors in a number of practical
applications. For example, in Chapter 8, we will see that there are a number of
different signals that can interrupt the execution of a program. We can selectively
enable or disable different signals by specifying a bit-vector mask, where a 1 in
bit position i indicates that signal i is enabled and a 0 indicates that it is disabled.
Thus, the mask represents the set of enabled signals.

One useful feature of C is that it supports bitwise Boolean operations. In fact, the
symbols we have used for the Boolean operations are exactly those used by C:
| for or, & for and, ~ for not, and ^ for exclusive-or. These can be applied to
any “integral” data type, including all of those listed in Figure 2.3. Here are some
examples of expression evaluation for data type char:

| C expression  | Binary expression           | Binary result | Hexadecimal result |
| ------------- | --------------------------- | ------------- | ------------------ |
| ~0x41         | ~[0100 0001]                | [1011 1110]   | 0xBE               |
| ~0x00         | ~[0000 0000]                | [1111 1111]   | 0xFF               |
| 0x69 & 0x55   | [0110 1001] & [0101 0101]   | [0100 0001]   | 0x41               |
| 0x69 `|` 0x55 | [0110 1001] `|` [0101 0101] | [0111 1101]   | 0x7D               |

As our examples show, the best way to determine the effect of a bit-level expression is to expand the hexadecimal arguments to their binary representations,
perform the operations in binary, and then convert back to hexadecimal.

One common use of bit-level operations is to implement masking operations,
where a mask is a bit pattern that indicates a selected set of bits within a word. As
an example, the mask 0xFF (having ones for the least significant 8 bits) indicates
the low-order byte of a word. The bit-level operation x & 0xFF yields a value
consisting of the least significant byte of x, but with all other bytes set to 0. For
example, with x = 0x89ABCDEF, the expression would yield 0x000000EF. The
expression ~0 will yield a mask of all ones, regardless of the size of the data
representation. The same mask can be written 0xFFFFFFFF when data type int is
32 bits, but it would not be as portable.

## 2.1.8 Logical Operations in C

C also provides a set of logical operators ||, &&, and !, which correspond to the
or, and, and not operations of logic. These can easily be confused with the bitlevel operations, but their behavior is quite different. The logical operations treat
any nonzero argument as representing true and argument 0 as representing false.
They return either 1 or 0, indicating a result of either true or false, respectively.
Here are some examples of expression evaluation:

| Expression     | Result |
| -------------- | ------ |
| !0x41          | 0x00   |
| !0x00          | 0x01   |
| !!0x41         | 0x01   |
| 0x69 && 0x55   | 0x01   |
| 0x69 `||` 0x55 | 0x01   |

Observe that a bitwise operation will have behavior matching that of its logical
counterpart only in the special case in which the arguments are restricted to 0
or 1.

A second important distinction between the logical operators ‘&&’ and ‘||’
versus their bit-level counterparts ‘&’ and ‘|’ is that the logical operators do not
evaluate their second argument if the result of the expression can be determined
by evaluating the first argument. Thus, for example, the expression a && 5/a will
never cause a division by zero, and the expression p && *p++ will never cause the
dereferencing of a null pointer.

## 2.1.9 Shift Operations in C

C also provides a set of shift operations for shifting bit patterns to the left and to
the right. For an operand x having bit representation [xw−1, xw−2,...,x0], the C
expression x << k yields a value with bit representation [xw−k−1, xw−k−2,...,x0,
0,..., 0]. That is, x is shifted k bits to the left, dropping off the k most significant
bits and filling the right end with k zeros. The shift amount should be a value
between 0 and w − 1. Shift operations associate from left to right, so x << j << k
is equivalent to (x << j) << k.There is a corresponding right shift operation, written in C as x >> k, but it has
a slightly subtle behavior. Generally, machines support two forms of right shift:

- Logical. A logical right shift fills the left end with k zeros, giving a result 
  [0,..., 0, xw−1, xw−2,...xk].
- Arithmetic. An arithmetic right shift fills the left end with k repetitions of the
  most significant bit, giving a result [xw−1,...,xw−1, xw−1, xw−2,...xk].
  This convention might seem peculiar, but as we will see, it is useful for
  operating on signed integer data.

As examples, the following table shows the effect of applying the different
shift operations to two different values of an 8-bit argument x:

| Operation           | Value 1      | Value 2      |
| ------------------- | ------------ | ------------ |
| Argument x          | `[01100011]` | `[10010101]` |
| x << 4              | `[00110000]` | `[01010000]` |
| x >> 4 (logical)    | `[00000110]` | `[00001001]` |
| x >> 4 (arithmetic) | `[00000110]` | `[11111001]` |

The italicized digits indicate the values that fill the right (left shift) or left (right
shift) ends. Observe that all but one entry involves filling with zeros. The exception
is the case of shifting `[10010101]` right arithmetically. Since its most significant bit
is 1, this will be used as the fill value.

The C standards do not precisely define which type of right shift should be
used with signed numbers—either arithmetic or logical shifts may be used. This
unfortunately means that any code assuming one form or the other will potentially
encounter portability problems. In practice, however, almost all compiler/machine
combinations use arithmetic right shifts for signed data, and many programmers
assume this to be the case. For unsigned data, on the other hand, right shifts must
be logical.

In contrast to C, Java has a precise definition of how right shifts should be
performed. The expression x >> k shifts x arithmetically by k positions, while
x >>> k shifts it logically.

Figure 2.14 shows the bit patterns and numeric values for several important
numbers for different word sizes. The first three give the ranges of representable
integers in terms of the values of UMaxw, TMinw, and TMaxw. We will refer
to these three special values often in the ensuing discussion. We will drop the
subscript w and refer to the values UMax, TMin, andTMax when w can be inferred
from context or is not central to the discussion.

A few points are worth highlighting about these numbers. First, as observed
in Figures 2.9 and 2.10, the two’s-complement range is asymmetric: |TMin| =
|TMax| + 1; that is, there is no positive counterpart to TMin. As we shall see, this
leads to some peculiar properties of two’s-complement arithmetic and can be the
source of subtle program bugs. This asymmetry arises because half the bit patterns
(those with the sign bit set to 1) represent negative numbers, while half (those
with the sign bit set to 0) represent nonnegative numbers. Since 0 is nonnegative,
this means that it can represent one less positive number than negative. Second,
the maximum unsigned value is just over twice the maximum two’s-complement
value: UMax = 2TMax + 1. All of the bit patterns that denote negative numbers in
two’s-complement notation become positive values in an unsigned representation. Figure 2.14 also shows the representations of constants −1 and 0. Note that −1
has the same bit representation as UMax—a string of all ones. Numeric value 0 is
represented as a string of all zeros in both representations.

The C standards do not require signed integers to be represented in two’scomplement form, but nearly all machines do so. Programmers who are concerned
with maximizing portability across all possible machines should not assume any
particular range of representable values, beyond the ranges indicated in Figure
2.11, nor should they assume any particular representation of signed numbers.
On the other hand, many programs are written assuming a two’s-complement
representation of signed numbers, and the “typical” ranges shown in Figures 2.9
and 2.10, and these programs are portable across a broad range of machines
and compilers. The file <limits.h> in the C library defines a set of constants delimiting the ranges of the different integer data types for the particular machine
on which the compiler is running. For example, it defines constants INT_MAX, INT_
MIN, and UINT_MAX describing the ranges of signed and unsigned integers. For a
two’s-complement machine in which data type int has w bits, these constants
correspond to the values of TMaxw, TMinw, and UMaxw.